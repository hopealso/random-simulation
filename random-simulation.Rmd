---
title: "Statistical Analysis, MSCA 31007, Lecture 2"
author: "Hope Foster-Reyes"
date: "October 7, 2016"
output: pdf_document
---

# Simulation of Random Variables

_Notes_

* _Style Guide:_ https://google.github.io/styleguide/Rguide.xml

* _Packages required: random, compositions, randtests, randtoolbox._

* _Files required: ScratchOffMonteCarlo.rda; store in RStudio project directory_

```{r settings, echo=FALSE}
options(scipen = 5)
```

## Part 1. Generate uniformly distributed random numbers

## 1.1. Use runif(), short for random (r) unified distribution (unif).

```{r q1.1}
# Simulate n pseudo-random numbers uniformly distributed on [a,b]
set.seed(15)
sample.runif <- runif(n = 1000, min = 0, max = 1)
str(sample.runif)
head(sample.runif)
quantile(sample.runif)
```

## 1.2. Simulate Uniform Random Sample on [0,1] Using Random.org.

```{r q1.2-load1, message=FALSE, warning=FALSE}
# Load random package to interface with random.org
library(random)
```

```{r q1.2}
# Download binary sequence from random.org
num.flips <- 1000
data.random.org <- randomNumbers(n = num.flips, min = 0, max = 1, 
                                 col = 1, base = 2, check = TRUE)
head(data.random.org)
```

**Let us turn our sequence of {0,1} into uniform random numbers on [0,1].**

```{r q1.2-load2, message=FALSE, warning=FALSE}
# Load compositions package
suppressMessages(library(compositions))
```

```{r q1.2-transform}
# Create function that interprets a sequence of zeros and ones of length n as a binary
#   number and converts that binary into decimal
BinaryToDec <- function(binary.seq){
  # Argument binary.seq: vector containing a binary sequence of 0's and 1's
  unbinary(paste(binary.seq, collapse = ""))
}

# Demonstrate function
BinaryToDec(c(1,1,1,1,1,0))

# Turn the sequence of 0's and 1's from random.org into a 100x10 matrix
matrix.binary <- matrix(data.random.org, ncol = 10)
head(matrix.binary)
str(matrix.binary)

# Transform each row into decimal format
vector.decimal <- apply(matrix.binary, 1, BinaryToDec)
str(vector.decimal)

# Divide the numbers by 2^10 to make real numbers in [0,1]
sample.random.org <- vector.decimal / 2^10
str(sample.random.org)
sample.random.org
quantile(sample.random.org)
```

All numbers in sample.random.org are between 0 and 1. This is our equivalent of the sample obtained by runif().

## Part 2. Test random number generator

## 2.1. Test uniformity of distribution of both random number generators

### 2.1.1. Sample obtained by runif()

Analyze what was simulated.

```{r q2-1-1-hist}
title.runif <- "1000 Pseudo-Random Numbers via Runif()"
hist.runif <- hist(sample.runif, main = title.runif)
hist.runif
boxplot(sample.runif, main = title.runif)
```

***What does the histogram tell you about the distribution? Is it consistent with the goal of simulation?***

Our goal was to simulate 1000 random numbers having a continuous uniform distribution between [0,1]. We have stored these 1000 numbers in a sample vector and graphed a histogram of this vector to examine whether we were successful.

As seen in the histogram, when we group our 1000 numbers into bins of width one tenth (0.1), each of these bins holds approximately 100 numbers, with some variation. This is consistent with our goal (i.e. 1000/10 = 100 and 1/10 = 0.1). Our sample of continuous numbers appears to be evenly distributed between the bounds of 0 and 1, as expected for a distribution of X ~ Unif(0,1), and appears consistent with the goal of our simulation.

```{r q2-1-1-spread}
# Estimate mean and standard deviation of our sample's density.
(hist.runif.mean <- mean(hist.runif$density))
(hist.runif.sd <- sd(hist.runif$density))

# Plot our histogram again, adding mean and standard deviation to assist in interpreting spread.
plot(hist.runif, freq = FALSE, main = paste("Variation of", title.runif))
abline(h = hist.runif.mean)
abline(h = hist.runif.mean + 1.96 * hist.runif.sd, col = "red", lty = 2)
abline(h = hist.runif.mean - 1.96 * hist.runif.sd, col = "red", lty = 2)
```

***What does the graph tell you about the observed distribution?***

The density of a histogram is calculated for each bin by dividing the number of observations in that bin by the width of the bin. Here, we can compare the density of our sample histogram's bins with the theoretical probability density of a uniform distribution.

We know that for a theoretical continuous uniform distribution, the probability density of each individual value inside its bounds is 1 divided by the distance between its maximum and minimum value. Thus for such a distribution between 0 and 1, the density for every value within its bounds will be 1 / (1-0), or simply 1.

For a sample taken from a continuous uniform distribution, we therefore would expect each of our bins to have a density close to 1, since the probability density of such a distribution remains at 1 throughout its range of values.

In other words, we know that our goal is a unified distribution on [0,1]. In a unified distribution on [0,1], as the size of the sample increases, the frequency of the outcomes should approach being equal, and their density should approach 1. 

Since the densities of our sample (read, the densities of the bins of our histogram) should approach 1 and be equal, their mean of course should also approach 1 and density across values (bins) should not deviate excessively.

Examining the empirical density of our sample as diagrammed on our histogram, we can see that the central tendency of our 10 bins is measured with mean of 1 and further our bin densities are within one standard deviation of 1. Thus the central tendency and variation of these bins are consistent with a continuous uniform distribution on [0,1] and our goal.

```{r q2-1-1-moments}
# Estimate the moments of the sample.
(sample.runif.mean <- mean(sample.runif))
(sample.runif.var <- var(sample.runif))
```

***What do you conclude about the estimated distribution from the moments?***

The first moment (mean) and the second central moment (variance) tell us, respectively, the center of our sample and the spread of our sample. These are also consistent with our goal. 

For a uniform distribution we expect that the center of our data will be midway through the range of the distribution. Thus the sample mean of `r sample.runif.mean` is quite close to our expected theoretical mean of 0.5 (midway between 0 and 1, or $\frac{(1-0)}{2}$. We expect that the variance of our data will be $\frac{(b-a)^{2}}{12}$, or 1/12, and our sample variance of `r sample.runif.var` is quite close to this.

```{r q2-1-1-iqr}
# Check the summary of the simulated sample.
summary(sample.runif)
```

***What do you think is the best way of estimating uniform distribution over unknown interval?***

Cosma Shalizi of [Carnegie Mellon University](http://www.stat.cmu.edu/~cshalizi/350/lectures/28/lecture-28.pdf) explains that the histogram is a rudimentary way of examining a distribution. "If we hold the bins fixed and take more and more data, then by the law of large numbers we anticipate that the relative frequency for each bin will converge on the bin’s probability."

We have used three different methods of examining our sample data:
* Examining the histogram of the data
* Examining the spread of the histogram's bins
* Examining the moments of the data itself
* Examining the interquartile range of the data (IQR)

If we define **"estimating a distribution"** as estimating, for a given data set, both its distribution (which distribution the data appears to take) and its parameters (the parameters for the distribution we suspect), are these methods helpful?

*How can we confirm that data is uniform?*

For any uniform distribution we know that the frequency and the density of its values should be uniform across its range. This is a distinct distribution, so eyeballing the histogram gives us a good sense of whether we can suspect a uniform distribution.

We have gone one step further by calculating the mean and standard deviation of our histogram's bins. In contrast with the mean and standard deviation of the data itself, these tell us about the extent to which our bin density varies from the expected mean of 1. This method seems particularly useful for identifying a uniform distribution as it answers the question, "Is the density across all values spread narrowly and close to 1?" If our density varies widely across our data points (i.e. the bins of our histogram), we know that the distribution cannot be uniform.

Thus, for identifying a uniform distribution, this is a more useful value than calculating the mean or standard deviation of the data itself. A uniform distribution spread across an unknown interval could have a very large standard deviation. But the standard deviation of its density (e.g. how much does the density of the bins themselves vary) will remain narrow.

The mean and standard deviation of the data itself can still give us a hint, as we do know how to calculate these values for a uniform distribution on any interval. However they alone cannot identify a uniform distribution, as demonstrated by the normal distribution below, which has a mean and standard deviation that matches our distribution on Unif(0,1).

```{r q2-1-1-compare}
sample.norm <- rnorm(1000, mean = 0.5, sd = sqrt(1/12))
summary(sample.norm)
hist.norm <- hist(sample.norm, main = "Normal Sample")

(paste("Unif mean: ", sample.runif.mean))
(paste("Unif variance: ", sample.runif.var))

(paste("Normal mean: ", mean(sample.norm)))
(paste("Normal variance: ", var(sample.norm)))
```

To identify a uniform distribution, it appears we need to look at not just the moments of the sample itself but also the variation of the calculated frequencies or densities across the data points.

*How can we estimate its parameters?*

Estimating the parameters of a uniform distribution is a different problem. We cannot estimate the minimum and maximum of a distribution in the same way that we identified the distribution. Knowing that the density of our histogram's bins is centered narrowly around zero will be true for any uniform distribution of any interval.

However, if we know the minimum, maximum, mean and standard deviation of the distribution, it seems we can come close to estimating the minimum and maximum. We may need to make some assumptions regarding just how far extended from the bounds of our sample the distribution in question may be.

### 2.1.2. Repeat the same steps to test uniformity of the sample created from Random.org data.

Analyze what was simulated.

```{r q2-1-2-hist}
title.random.org <- "100 Pseudo-Random Numbers,\nTransformed from Random.org Binary Data"
hist.random.org <- hist(sample.random.org, main = title.random.org)
hist.random.org
boxplot(sample.random.org, main = title.random.org)
```

***What does the histogram tell you about the distribution? Is it consistent with the goal of simulation?***

In this case, our goal was to simulate 100 random numbers having a continuous uniform distribution between [0,1], and we have used a binary sequence from a physical source, provided by the website Random.org, to produce our sample.

As seen in the histogram, when we group our 100 numbers into bins of width one tenth (0.1), each of these bins holds between 5 and 15 numbers. We expect each bin to hold approximately 10 numbers (i.e. 100/10 = 10 and 1/10 = 0.1). Our smaller sample based on Random.org appears to hold more variation, but the central tendency of our bin frequency, which appears to be approximately 10 from observing the histogram, is consistent with our goal.

```{r q2-1-2-spread}
# Estimate mean and standard deviation of our sample's density.
(hist.random.org.mean <- mean(hist.random.org$density))
(hist.random.org.sd <- sd(hist.random.org$density))

plot(hist.random.org, freq = FALSE, main = paste("Variation of", title.random.org))
abline(h = hist.random.org.mean)
abline(h = hist.random.org.mean + 1.96 * hist.random.org.sd, col = "red", lty = 2)
abline(h = hist.random.org.mean - 1.96 * hist.random.org.sd, col = "red", lty = 2)
```

***What does the graph tell you about the observed distribution?***

Once again, Examining the empirical density of our sample as diagrammed on our histogram, we can see that the central tendency of our 10 bins is measured with mean of 1 and further our bin densities are within one standard deviation of 1. Though this smaller sample of 100 shows more variation than our earlier sample of 1000, the central tendency and variation of these bins remains approximately consistent with a continuous uniform distribution on [0,1] and our goal. 

```{r q2-1-2-moments}
# Estimate the moments of the sample.
(sample.random.org.mean <- mean(sample.random.org))
(sample.random.org.var <- var(sample.random.org))
```

***What do you conclude about the estimated distribution from the moments?***

For a uniform distribution we expect that the center of our data will be midway through the range of the distribution. Thus the sample mean of `r sample.random.org.mean` is quite close to our expected theoretical mean of 0.5 (midway between 0 and 1, or $\frac{(1-0)}{2}$. We expect that the variance of our data will be $\frac{(b-a)^{2}}{12}$, or 1/12, and our sample variance of `r sample.random.org.var` is quite close to this.

```{r q2-1-2-iqr}
# Check the summary of the simulated sample.
summary(sample.random.org)
```

Using the same methods to examine our random.org sample as we did our runif sample, we noted that the latter, being a sample of 100 rather than 1000, resulted in greater variation and demonstrated less strongly the expected parameters of a uniform distribution. However, given than 100 is a relatively small sample size, we suspect that given the law of large numbers that the variations we saw would even themselves out with a larger sample.

## 2.2. Test independence of the sequence of zeros and ones

### 2.2.1. Turning point test

Check if a sequence of numbers is i.i.d. (independent identically distributed) based on the number of turning points in the sequence.

The number of turning points is the number of maxima and minima in the series. The statistic of the test has a standard normal distribution and is performed by turning.point.test() in package randtests.

```{r q2-2-1-load, message=FALSE, warning=FALSE}
library(randtests)
```

```{r q2-2-1}
turning.point.test(sample.random.org)
```

The null hypothesis tested by turning point test is randomness (i.i.d.). The alternative is serial correlation in the sequence. Thus, if the test returns a very small p-value the randomness needs to be rejected.

## 2.3. Test frequency by Monobit test

To perform Monobit test you need to transform your {0,1} sample into {-1,1}.

```{r q2-3-transform}
data.random.org.plusminus1 <- (data.random.org - .5) * 2
```

The monobit test can be calculated in R with the help of pnorm. The test checks that the the p-value or complimentary error function of a statistic S is less than or equal to 0.01. If not, the sequence fails the test.

```{r q2-3}
erf.monobit <- function(x) 2 * pnorm(x * sqrt(2)) - 1
erfc.monobit <- function(x) 2 * pnorm(x * sqrt(2), lower = FALSE)

# Chart the complimentary error function
plot(seq(from = -3, to = 3, by = 0.5), erfc.monobit(seq(from = -3, to = 3, by = 0.5)),
     type = "l", xlab = "x", ylab = "erfc.monobit(x)")

# Calculate our S statistic
s.monobit.random.org <- abs(sum(data.random.org.plusminus1) / sqrt( 2* num.flips))

# Find the p-value or erfc(S)
erfc.monobit(s.monobit.random.org)
```

The test shows that our sequence passes.

Now check each of the sub-sequences created earlier:

```{r q2-3-subsseq}
# Create matrix of sub-sequences
matrix.plusminus1 <- matrix(data.random.org.plusminus1, ncol=50)

# Find p-value of vector of S statistics for every sub-sequence
erfc.random.org.subseq <- erfc.monobit(abs(apply(matrix.plusminus1, 1, sum)) / sqrt(2 * 50))

# Plot
plot(erfc.random.org.subseq, ylab = "P-Values", 
     main = "Monobit Complimentary Error Function on 20 Sequences of Random.org Data")
```

***How many runs out of 20 fail the test?***

```{r q2-3-subseq-test}
sum(erfc.random.org.subseq <= 0.01)
```

We can confirm that the random sequences provided by Random.org pass the monobit test.

## Part 3. Invent a random number generator

Our goal in inventing a random number generator is to seek out a true source of a uniform random sequence on [0,1]. One random event that is available all of those who live in a metropolitan area is the movement of cars on the roads that surround us. To generate our random sequence we combined the draw of statistics with the draw to spend time with our family, and spent two hours recording the movement of cars along a relatively busy throughfare in the North Center neighborhood of Chicago.

Let's bring this data, recorded in Excel, into R.

```{r q3-import}

```


```{r q3}
sample.unif1 <- runif(1000, 0, 1)
hist(sample.unif1)
sample.binom <- qbinom(sample.unif1, size = 1, prob = 0.8)
hist(sample.binom)
sample.unif2 <- punif(sample.binom, 0, 1)
hist(sample.unif2)
```


## Part 4. Monte Carlo Method

## 4.1. Scratch off quote of the day: fuction download

```{r q4-1-load, message=FALSE, warning=FALSE}
filename <- "ScratchOffMonteCarlo.rda"

#path <- "{Path to the folder with ScratchOffMonteCarlo.rda}"
#load(file = paste(path, filename,sep= '/ '))

# Place ScratchOffMonteCarlo file in project directory or use code above
load(filename)
```

## 4.2. Simulate pseudo-random poins [x,y][x,y] on [0,100]×[0,100]

The ScratchOffMonteCarlo function simulates 'scratching off the paint' covering an image at each x, y coordinate. Let's try creating 1000 pseudo-random x, y coordinates and see if the image is readable.

```{r q4-1-example}
# Set seed
seed1 <- 938364
set.seed(seed1)

# Set sample size
mc.n <- 1000

# Simulate n * 2 pseudo-random numbers uniformly distributed on [0, 100]
mc.xy <- runif(2 * mc.n, min = 0, max = 100)

# Transform into n x 2 matrix
mc.xy <- matrix(mc.xy, ncol = 2)
str(mc.xy)
head(mc.xy)

# Simulate 'scratching off the paint' covering an image at each x, y coordinate
ScratchOffMonteCarlo(mc.xy)
```

Let's adjust the sample size and seed and try to make the quote in our image readable with minimum sample size.

```{r q4-1-trials}
# Create function to generate coordinate matrix
xy <- function(n, seed) {
  # Argument n: Number of x, y coordinate samples to generate
  # Argument seed: Seed to ensure reproducibility
  
  # Simulate n * 2 pseudo-random numbers uniformly distributed on [0, 100]
  set.seed(seed)
  xy <- runif(2 * n, min = 0, max = 100)

  # Transform into n x 2 matrix
  xy <- matrix(xy, ncol = 2)
}

# Seed 1 5k:
mc.n <- 5000
mc5k.xy <- xy(mc.n, seed1)
ScratchOffMonteCarlo(mc5k.xy)

# Seed 2 5k:
seed2 <- 112233
mc5k.xy <- xy(mc.n, seed2)
ScratchOffMonteCarlo(mc5k.xy)

# Seed 1 10k:
mc.n <- 10000
mc10k.xy <- xy(mc.n, seed1)
ScratchOffMonteCarlo(mc10k.xy)

# Seed 2 10k:
mc10k.xy <- xy(mc.n, seed2)
ScratchOffMonteCarlo(mc10k.xy)

# Seed 1 15k:
mc.n <- 15000
mc15k.xy <- xy(mc.n, seed1)
ScratchOffMonteCarlo(mc15k.xy)

# Seed 2 15k:
mc15k.xy <- xy(mc.n, seed2)
ScratchOffMonteCarlo(mc15k.xy)

# Seed 3 15k:
seed3 <- 999999
mc15k.xy <- xy(mc.n, seed3)
ScratchOffMonteCarlo(mc15k.xy)

# Seed 2 19k:
mc.n <- 19000
mc19k.xy <- xy(mc.n, seed2)
ScratchOffMonteCarlo(mc19k.xy)

# Seed 2 23k:
mc.n <- 23000
mc23k.xy <- xy(mc.n, seed2)
ScratchOffMonteCarlo(mc23k.xy)

# Seed 2 25k:
mc.n <- 25000
mc25k.xy <- xy(mc.n, seed2)
ScratchOffMonteCarlo(mc25k.xy)

# Seed 1 25k:
mc25k.xy <- xy(mc.n, seed1)
ScratchOffMonteCarlo(mc25k.xy)

# Seed 3 25k:
mc25k.xy <- xy(mc.n, seed3)
ScratchOffMonteCarlo(mc25k.xy)
```

***What percent you needed to scratch off to make the quote readable?***

It appears we need to scratch off approximately 90% of the paint to make the quote and its author readable. (We can read the English words at less due to the cognitive ability to fill in letters in words that match existing patterns, but unknown names require more letters to be readable.)

## 4.3. Simulate quasi-random poins [x,y][x,y] on [0,100]×[0,100]

We replace function runif() with sobol() from the library randtoolbox.

```{r q4-3-load, message=FALSE, warning=FALSE}
library(randtoolbox)
```

```{r}
seed4 <- 10
set.seed(seed4)
mc.n <- 10
mc10.xy <- sobol(mc.n, dim = 2, init = T) * 100
```


Again, by changing nSample and my.seed we try to make the quote of the day readable with minimum sample size.

***What percent you needed to scratch off to make the quote readable?***

***Which of the Monte Carlo methods makes the quote readable sooner?***

***Which parameters nSample and my.seed gave you the best result, what percent of the yellow paing you were able to scratch off by each method?***

***Changing which of the two parameters plays more significant role?***

## Workshop: Simulation of random variables using distribution function and inverse distribution function

```{r inv-function-norm}
xNorm<-qnorm((1:99)/100)
yNorm<-pnorm(xNorm)
plot(xNorm,yNorm,pch=16)
abline(h=yNorm)
abline(v=xNorm)
abline(h=yNorm[30],col="green",lwd=3)
abline(v=xNorm[30],col="green",lwd=3)
lines(xNorm,dnorm(xNorm),lwd=3,col="red")
```

## Part 5. Test

```{r q5}
# Load data. (Data file must be located in RStudio project directory.)
data <- read.csv('Week2_Test_Sample.csv', header = TRUE)$x

# Using sample from uniform distribution, create sample from normal distribution. 
data.norm <- qnorm(data[4:503], mean = data[1], sd = data[2])
hist(data.norm)

# Using sample from uniform distribution, create sample from exponential distribution.
data.exp <- qexp(data[4:503], rate = data[3])
hist(data.exp)

# Output results to RStudio project directory
result <- cbind(datNorm = data.norm, datExp = data.exp)
write.csv(result, 'result.csv', row.names = FALSE)
```

